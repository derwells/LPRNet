{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss\n",
    "\n",
    "def conv2D_batchnorm(*args, **kwargs):\n",
    "    return keras.Sequential([\n",
    "        layers.Conv2D(*args, **kwargs),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU()\n",
    "    ])\n",
    "\n",
    "def basic_block(channel_out):\n",
    "    return keras.Sequential([\n",
    "        conv2D_batchnorm(\n",
    "            channel_out//4, 1, strides=1, padding=\"same\"\n",
    "        ),\n",
    "        conv2D_batchnorm(\n",
    "            channel_out//4, (3, 1), strides=1, padding=\"same\"\n",
    "        ),\n",
    "        conv2D_batchnorm(\n",
    "            channel_out//4, (1, 3), strides=1, padding=\"same\"\n",
    "        ),\n",
    "        conv2D_batchnorm(\n",
    "            channel_out//4, 1, strides=1, padding=\"same\"\n",
    "        ),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class global_context(layers.Layer):\n",
    "    def __init__(self, ksize, strides):\n",
    "        super().__init__()\n",
    "        self.ksize = ksize\n",
    "        self.strides = strides\n",
    "    \n",
    "    def call(self, channel_in):\n",
    "        x = layers.AveragePooling2D(\n",
    "            pool_size=self.ksize, strides=self.strides, padding=\"same\"\n",
    "        )(channel_in)\n",
    "        cx = layers.Lambda(lambda e: tf.math.square(e))(x)\n",
    "        cx = layers.Lambda(lambda e: tf.math.reduce_mean(e))(cx)\n",
    "        return layers.Lambda(\n",
    "            lambda e: tf.math.divide(e[0], e[1])\n",
    "        )([x, cx])\n",
    "\n",
    "def lprnet(\n",
    "    n_classes,\n",
    "    shape=(24, 94, 3),\n",
    "):\n",
    "    input_layer = layers.Input(shape)\n",
    "    x = conv2D_batchnorm(\n",
    "        64, (3, 3), strides=1, padding=\"same\"\n",
    "    )(input_layer)\n",
    "    x = layers.MaxPool2D(pool_size=(3, 3), strides=1)(x)\n",
    "    x2 = basic_block(128)(x)\n",
    "    x = layers.MaxPool2D(pool_size=(3, 3), strides=(1, 2))(x2)\n",
    "    x = basic_block(256)(x)\n",
    "    x3 = basic_block(256)(x)\n",
    "    x = layers.MaxPool2D(pool_size=(3, 3), strides=(1, 2))(x3)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = conv2D_batchnorm(\n",
    "        256, (4, 1), strides=1, padding=\"same\"\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = conv2D_batchnorm(\n",
    "        n_classes, (1, 13), strides=1, padding=\"same\"\n",
    "    )(x)\n",
    "\n",
    "    \"\"\"\n",
    "    # Global Context\n",
    "    cx = layers.Lambda(lambda e: tf.math.square(e))(x)\n",
    "    cx = layers.Lambda(lambda e: tf.math.reduce_mean(e))(cx)\n",
    "    x0  = layers.Lambda(\n",
    "        lambda e: tf.math.divide(e[0], e[1])\n",
    "    )([x, cx])\n",
    "\n",
    "    x1 = global_context(\n",
    "        ksize=[1, 4],\n",
    "        strides=[1, 4],\n",
    "    )(input_layer)\n",
    "    x2 = global_context(\n",
    "        ksize=[1, 4],\n",
    "        strides=[1, 4],\n",
    "    )(x2)\n",
    "    x3 = global_context(\n",
    "        ksize=[1, 2],\n",
    "        strides=[1, 2],\n",
    "    )(x3)\n",
    "\n",
    "    x = layers.Lambda(\n",
    "        lambda e: tf.concat([e[0], e[1], e[2], e[3]], 3)\n",
    "    )([x1, x2, x3, x0])\n",
    "    x = layers.Conv2D(\n",
    "        n_classes,\n",
    "        kernel_size=(1,1),\n",
    "        strides=(1,1),\n",
    "    )(x)\n",
    "    logits = layers.Lambda(\n",
    "        lambda x: tf.math.reduce_mean(x, axis=1)\n",
    "    )(x)\n",
    "    output_layer = layers.Softmax()(logits)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return keras.Model(input_layer, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_OUTPUTS: 35\n"
     ]
    }
   ],
   "source": [
    "CHAR_OUTPUTS = \"ABCDEFGHJKLMNPQRSTUVWXYZ0123456789\"\n",
    "CHAR_ENCODE = {c:i for i, c in enumerate(CHAR_OUTPUTS)}\n",
    "N_OUTPUTS = len(CHAR_OUTPUTS) + 1\n",
    "print(f\"N_OUTPUTS: {N_OUTPUTS}\")\n",
    "\n",
    "model = lprnet(N_OUTPUTS)\n",
    "learning_rate_scheduler = keras.optimizers.schedules.ExponentialDecay(\n",
    "    1e-3, \n",
    "    decay_steps=500,\n",
    "    decay_rate=0.995,\n",
    "    staircase=True\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate_scheduler\n",
    "    ),\n",
    "    loss=ctc_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 24, 94, 3)]       0         \n",
      "                                                                 \n",
      " sequential_72 (Sequential)  (None, 24, 94, 64)        2048      \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 22, 92, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " sequential_77 (Sequential)  (None, 22, 92, 32)        9856      \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 20, 45, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " sequential_82 (Sequential)  (None, 20, 45, 64)        32000     \n",
      "                                                                 \n",
      " sequential_87 (Sequential)  (None, 20, 45, 64)        34048     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 18, 22, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 18, 22, 64)        0         \n",
      "                                                                 \n",
      " sequential_88 (Sequential)  (None, 18, 22, 256)       66816     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 18, 22, 256)       0         \n",
      "                                                                 \n",
      " sequential_89 (Sequential)  (None, 18, 22, 35)        116655    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 261,423\n",
      "Trainable params: 259,433\n",
      "Non-trainable params: 1,990\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = tf.random.uniform((10, 24, 94, 3))\n",
    "test_output = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generators.plates_generator import ImageGenerator\n",
    "\n",
    "generator = ImageGenerator()\n",
    "data, labels = generator.generate_images(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.axis(False)\n",
    "plt.imshow(data[0], interpolation='nearest')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d729c60030d22124f309314ed8b536bbf05fb9d62d10f80118cbfedc7320c2c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('python-3.8.10': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
